{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f23764-2f4f-4de4-bcb2-7bcf1246a19f",
   "metadata": {},
   "source": [
    "# Part1: Language Modeling / Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97b2d73-45b9-46f9-aeb3-f9672ed0f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ayoubbakkali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ayoubbakkali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ayoubbakkali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca81a1-86f2-4b66-8bf2-ee1a86fda18b",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7790e400-a915-4dfe-95f6-3c8dbffdde0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High risk problems are address in the prototyp...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To simulate portions of the desired final prod...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A prototype program simulates the behaviors of...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Defined in the Specification phase a prototype...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is used to let the users have a first idea ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  score\n",
       "0  High risk problems are address in the prototyp...    3.5\n",
       "1  To simulate portions of the desired final prod...    5.0\n",
       "2  A prototype program simulates the behaviors of...    4.0\n",
       "3  Defined in the Specification phase a prototype...    5.0\n",
       "4  It is used to let the users have a first idea ...    3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/dbbrandt/short_answer_granding_capstone_project/master/data/sag/answers.csv\")\n",
    "dataset = dataset.drop([\"correct\", \"id\"], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40be5d-dd3a-4a31-b521-eef1dd9d9611",
   "metadata": {},
   "source": [
    "## 2. NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bae745b-fd9f-4530-ab01-49950a32595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4684305b-1f3f-4dd7-84f5-80a89df3f15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [High, risk, problems, are, address, in, the, ...\n",
       "1    [To, simulate, portions, of, the, desired, fin...\n",
       "2    [A, prototype, program, simulates, the, behavi...\n",
       "3    [Defined, in, the, Specification, phase, a, pr...\n",
       "4    [It, is, used, to, let, the, users, have, a, f...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"tokens\"] = dataset[\"answer\"].apply(word_tokenize)\n",
    "dataset[\"tokens\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befb261d-2f38-496e-97a3-339851fbfe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [High, risk, problems, address, prototype, pro...\n",
       "1       [simulate, portions, desired, final, product, ...\n",
       "2       [prototype, program, simulates, behaviors, por...\n",
       "3       [Defined, Specification, phase, prototype, sti...\n",
       "4       [used, let, users, first, idea, completed, pro...\n",
       "                              ...                        \n",
       "2437                                             [log, n]\n",
       "2438                               [minus, 1, divided, 2]\n",
       "2439                                               [2n-1]\n",
       "2440             [takes, h, steps, ,, h, height, tree, .]\n",
       "2441    [depends, install, search, tree, whatever, cas...\n",
       "Name: tokens, Length: 2442, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"tokens\"] = dataset[\"tokens\"].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "dataset[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22ab0b7-028d-46a3-a0d7-98328832a2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [High, risk, problem, address, prototype, prog...\n",
       "1       [simulate, portion, desired, final, product, q...\n",
       "2       [prototype, program, simulates, behavior, port...\n",
       "3       [Defined, Specification, phase, prototype, sti...\n",
       "4       [used, let, user, first, idea, completed, prog...\n",
       "                              ...                        \n",
       "2437                                             [log, n]\n",
       "2438                               [minus, 1, divided, 2]\n",
       "2439                                               [2n-1]\n",
       "2440               [take, h, step, ,, h, height, tree, .]\n",
       "2441    [depends, install, search, tree, whatever, cas...\n",
       "Name: lemmatization, Length: 2442, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dataset[\"lemmatization\"] = dataset[\"tokens\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x] )\n",
    "#dataset[\"lemmatization\"] = dataset[\"tokens\"].apply(lambda x: [word for word in x if word is not \"\" or word is not \" \"])\n",
    "dataset[\"lemmatization\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef22f5-65ba-48d5-a3ee-ec363e52faf2",
   "metadata": {},
   "source": [
    "### 3. Encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdf8be-a426-482e-ba5b-a254d7153d49",
   "metadata": {},
   "source": [
    "#### 3.1 Word2Vec (CBOW and Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259aff15-ca85-4576-b8a4-6638a0d63322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Train Word2Vec models (CBOW and Skip-gram)\n",
    "cbow_model = Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "skipgram_model = Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efbc351-70e6-4778-b0bd-e10d629eeb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442\n",
      "2442\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    # Get vectors for words in the sentence, ignore words not in the model's vocabulary\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if not word_vectors:  # If no words in the sentence are in the vocabulary, return a zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Compute the mean of the word vectors\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "cbow_vectors = np.array([get_sentence_embedding(sentence, cbow_model) for sentence in dataset['tokens']])\n",
    "skipgram_vectors = np.array([get_sentence_embedding(sentence, skipgram_model) for sentence in dataset['tokens']])\n",
    "\n",
    "\n",
    "print(len(cbow_vectors))\n",
    "print(len(skipgram_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f05d-f42b-4da4-8ac5-e4098c14e172",
   "metadata": {},
   "source": [
    "#### 3.2 BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175d7072-fac1-4ea6-95f2-6b6d48ca0567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2442, 2620)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "bow_matrix = vectorizer.fit_transform(dataset['answer'])\n",
    "bow_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2046f-ce26-40ae-b397-5d8ce62bc485",
   "metadata": {},
   "source": [
    "#### 3.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d713cf6-7ee3-4401-bc32-599b6554dd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2442x2620 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 34175 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['answer'])\n",
    "\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b7cc0-de9e-417e-8ae2-528bb4730a95",
   "metadata": {},
   "source": [
    "### 4. Model Training (Word2vect embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7324fcda-660f-488a-af90-38b12baba073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1699cd6f-b523-41a5-acf2-911e94a32e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07440589  0.08031797 -0.03426092 ... -0.38296026 -0.00309836\n",
      "   0.07928286]\n",
      " [-0.06286704  0.06985551 -0.02598914 ... -0.33443388 -0.00218877\n",
      "   0.06549644]\n",
      " [-0.06556115  0.07073511 -0.02641777 ... -0.33607349 -0.00262122\n",
      "   0.06618875]\n",
      " ...\n",
      " [-0.00713767 -0.00135911  0.00883542 ...  0.00480326 -0.00664322\n",
      "  -0.00177323]\n",
      " [-0.09801418  0.11211058 -0.04483525 ... -0.49658155 -0.00109324\n",
      "   0.10569453]\n",
      " [-0.07336561  0.08024236 -0.03486623 ... -0.3745698   0.00202734\n",
      "   0.07118678]]\n",
      "[3.5 5.  4.  ... 2.5 5.  1.5]\n",
      "(2442,)\n",
      "(2442, 100)\n"
     ]
    }
   ],
   "source": [
    "X, y = np.array(cbow_vectors),np.array(dataset[\"score\"])\n",
    "print(X)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711c42d3-4656-4048-96ed-f25ce756f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y , test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "117a3479-f1cf-4b72-9f66-e47ca1c798a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR MSE: 1.0104627475833199\n",
      "SVR RMSE: 1.005217761275297\n",
      "Linear Regression MSE: 105.78760965898223\n",
      "Linear Regression RMSE: 10.285310382238459\n",
      "Decision Tree Regressor MSE: 1.8832834867075663\n",
      "Decision Tree Regressor RMSE: 1.3723277621281171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SVR Model\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_predictions = svr_model.predict(X_test)\n",
    "svr_mse = mean_squared_error(y_test, svr_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "print(\"SVR MSE:\", svr_mse)\n",
    "print(\"SVR RMSE:\", svr_rmse)\n",
    "\n",
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_predictions)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "print(\"Linear Regression MSE:\", lr_mse)\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "\n",
    "# Decision Tree Regressor model\n",
    "dt_regressor_model = DecisionTreeRegressor()\n",
    "dt_regressor_model.fit(X_train, y_train)\n",
    "dt_regressor_predictions = dt_regressor_model.predict(X_test)\n",
    "dt_regressor_mse = mean_squared_error(y_test, dt_regressor_predictions)\n",
    "dt_regressor_rmse = np.sqrt(dt_regressor_mse)\n",
    "print(\"Decision Tree Regressor MSE:\", dt_regressor_mse)\n",
    "print(\"Decision Tree Regressor RMSE:\", dt_regressor_rmse)\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "# Note: Accuracy is not a typical metric for regression tasks\n",
    "svr_accuracy = svr_model.score(X_test, y_test)\n",
    "lr_accuracy = lr_model.score(X_test, y_test)\n",
    "dt_regressor_accuracy = dt_regressor_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e62150-64ce-4263-b852-3dcf124bd3be",
   "metadata": {},
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569656e-7af8-43c7-89b5-34a33a364126",
   "metadata": {},
   "source": [
    "After evaluating the performances of Support Vector Regressor (SVR), Linear Regression, and Decision Tree models, several important conclusions can be drawn. Firstly, in terms of average prediction accuracy, SVR exhibits the lowest Mean Squared Error (MSE) among the three models, closely followed by the Decision Tree. This suggests that both SVR and Decision Tree models better fit the data compared to Linear Regression, which has a significantly higher MSE. When examining the dispersion of prediction errors, measured by Root Mean Squared Error (RMSE), SVR and Decision Tree show relatively similar values, indicating similar dispersion of errors around the mean value. In contrast, Linear Regression demonstrates a notably higher RMSE, implying greater variability in prediction errors. Consequently, the choice of model depends on the specific project objectives, performance requirements, and data characteristics. In this case, although SVR and Decision Tree models exhibit relatively similar performances, the final model selection should be based on comprehensive evaluation considering various aspects including accuracy, complexity, and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c025d7f-758e-44d6-8026-7535b0f9965a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
